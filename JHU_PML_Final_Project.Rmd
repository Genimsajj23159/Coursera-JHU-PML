---
title: "JHU PML Final Project: Human Activity Recognition Using Accelerometers"
author: "Jasmine"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Executive Summary
This study aims to classify human exercise performance using accelerometer data collected from different body parts. The dataset consists of motion sensor readings from the belt, forearm, arm, and dumbbell of six individuals performing weightlifting exercises in five different ways. The goal is to develop a machine learning model that can accurately predict the execution manner (`classe`).

Several classification models were trained and evaluated, including Random Forest, Gradient Boosting Machine (GBM), Support Vector Machine (SVM), and Neural Networks. The models were compared based on accuracy using a validation set, and the best-performing model was chosen for final predictions. Random Forest emerged as the most effective model, offering high accuracy and robustness. The final predictions were made on a separate test set, demonstrating the applicability of the model for human activity recognition.

## Data Description
The dataset originates from accelerometer readings captured during weightlifting exercises performed by six participants. The available features include:
- **Timestamp and User Information:** Identifiers for observations and participants.
- **Sensor Readings:** A variety of numerical measurements from accelerometers on the belt, forearm, arm, and dumbbell, capturing movement dynamics.
- **Target Variable (`classe`):** A categorical variable indicating the manner in which the exercise was performed, with five distinct classes (A, B, C, D, E) representing different execution techniques, including correct and incorrect forms.

The dataset underwent preprocessing to remove missing values and irrelevant columns before model training. Various machine learning techniques were applied, and performance was assessed to determine the most suitable approach for classification.

## Load Libraries
```{r}
library(caret)
library(randomForest)
library(ggplot2)
library(dplyr)
library(gbm)
library(e1071)
library(nnet)
library(kernlab)
```

## Load Data
```{r}
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train_data <- read.csv(train_url, na.strings = c("NA", "", "#DIV/0!"))
test_data <- read.csv(test_url, na.strings = c("NA", "", "#DIV/0!"))
```

## Data Preprocessing
### Remove NA Columns and Unnecessary Features
```{r}
train_data <- train_data[, colSums(is.na(train_data)) == 0]
test_data <- test_data[, colSums(is.na(test_data)) == 0]

# Remove non-predictive columns
train_data <- train_data[, -c(1:7)]
test_data <- test_data[, -c(1:7)]
```

## Exploratory Data Analysis
### Check Class Distribution
```{r}
ggplot(train_data, aes(x = classe)) + 
  geom_bar(fill = "blue", alpha = 0.7) + 
  theme_minimal() + 
  labs(title = "Class Distribution", x = "Class", y = "Count")
```

## Split Data into Training and Validation Sets
```{r}
set.seed(123)
trainIndex <- createDataPartition(train_data$classe, p = 0.7, list = FALSE)
train_set <- train_data[trainIndex, ]
valid_set <- train_data[-trainIndex, ]

# Ensure classe is a factor
train_set$classe <- as.factor(train_set$classe)
valid_set$classe <- as.factor(valid_set$classe)
```

## Train Multiple Models
### Random Forest
```{r}
set.seed(123)
rf_model <- randomForest(classe ~ ., data = train_set, ntree = 100)
print(rf_model)
```

### Gradient Boosting Machine (GBM)
```{r}
set.seed(123)
gbm_model <- train(classe ~ ., data = train_set, method = "gbm",
                   trControl = trainControl(method = "cv", number = 3),
                   tuneGrid = expand.grid(interaction.depth = c(1, 3),
                                          n.trees = c(50, 100),
                                          shrinkage = 0.1,
                                          n.minobsinnode = 10),
                   verbose = FALSE)
print(gbm_model)
```

### Support Vector Machine (SVM)
```{r}
set.seed(123)
svm_model <- train(classe ~ ., data = train_set, method = "svmRadial",
                   trControl = trainControl(method = "cv", number = 3),
                   tuneLength = 3)
print(svm_model)
```

### Neural Network
```{r}
set.seed(123)
nnet_model <- train(classe ~ ., data = train_set, method = "nnet", trace = FALSE)
print(nnet_model)
```

## Model Evaluation
### Accuracy on Validation Set
```{r}
pred_valid_rf <- predict(rf_model, valid_set)
conf_matrix_rf <- confusionMatrix(pred_valid_rf, valid_set$classe)
print(conf_matrix_rf)

pred_valid_gbm <- predict(gbm_model, valid_set)
conf_matrix_gbm <- confusionMatrix(pred_valid_gbm, valid_set$classe)
print(conf_matrix_gbm)

pred_valid_svm <- predict(svm_model, valid_set)
conf_matrix_svm <- confusionMatrix(pred_valid_svm, valid_set$classe)
print(conf_matrix_svm)

pred_valid_nnet <- predict(nnet_model, valid_set)
conf_matrix_nnet <- confusionMatrix(pred_valid_nnet, valid_set$classe)
print(conf_matrix_nnet)
```

## Expected Out-of-Sample Error
```{r}
errors <- data.frame(
  Model = c("Random Forest", "GBM", "SVM", "Neural Network"),
  Error = 1 - c(conf_matrix_rf$overall["Accuracy"],
                conf_matrix_gbm$overall["Accuracy"],
                conf_matrix_svm$overall["Accuracy"],
                conf_matrix_nnet$overall["Accuracy"])
)
print(errors)
```
From the obtained error results:  

| Model            | Error        |
|-----------------|-------------|
| Random Forest   | 0.0051 (0.51%) |
| GBM             | 0.0551 (5.51%) |
| SVM             | 0.0734 (7.34%) |
| Neural Network  | 0.6051 (60.51%) |

The best model is **Random Forest**, as it has the lowest error (0.51%) compared to the other models. This indicates that Random Forest has the highest accuracy and is the best choice for predicting the **manner of exercise** in this case.

## Best Model Selection and Predictions on Test Data
```{r}
best_model <- rf_model  # Assuming Random Forest performed best; modify as needed
pred_test <- predict(best_model, test_data)
pred_test
```

## Conclusion
Multiple models were tested, including Random Forest, GBM, SVM, and Neural Networks. The best-performing model was selected based on validation accuracy and used to generate test predictions for submission.
